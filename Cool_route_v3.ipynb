{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/swaminaathakrishnan/Cool_Route_prototype/blob/master/Cool_route_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEQqXNM0C1M6"
   },
   "source": [
    "# üö¥ **CoolRide V3: AI-Driven Thermal Routing System**\n",
    "### Project Overview\n",
    "\n",
    "CoolRide is a TRL-6 prototype designed to route cyclists through thermally comfortable paths in Singapore. It utilizes real-time government weather data (NEA), satellite imagery (Vegetation), and predictive AI to mitigate Urban Heat Island (UHI) effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d0EGg6-lCtkh",
    "outputId": "93348b7c-9f6a-4cf4-9b16-add64925b8d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: celery 4.2.0 has a non-standard dependency specifier pytz>dev. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of celery or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m‚úÖ System Initialized. Ready for V3 Execution.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# üß± MODULE 1: SYSTEM INITIALIZATION\n",
    "# ==========================================\n",
    "# Objective: Install geospatial libraries and set up the environment.\n",
    "# dependencies: OSMnx (Maps), GeoPandas (Spatial Data), Scikit-Learn (AI).\n",
    "\n",
    "!pip install osmnx simplekml geopandas shapely networkx requests scikit-learn -q\n",
    "\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import simplekml\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from shapely.geometry import Point, LineString, box\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"‚úÖ System Initialized. Ready for V3 Execution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAdgf4rmDPgv"
   },
   "source": [
    "### ‚öôÔ∏è Module 2: Configuration & Cloud Connection\n",
    "Objective: Define the pilot zone and connect to the GitHub Data Lake. Logic: Instead of local files, we stream GeoJSON/CSV directly from the raw GitHub URLs. This allows the team to collaborate without sharing Drive folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RTJjWFUyDNOX",
    "outputId": "f0d0c5db-7dce-4a86-fb7f-ae7df1a55a6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration Loaded. Connecting to Data Lake at: swaminaathakrishnan/Cool_Route_prototype\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# ‚öôÔ∏è MODULE 2: CONFIGURATION\n",
    "# ==========================================\n",
    "\n",
    "# 1. PILOT ZONE\n",
    "PLACE_NAME = \"Tampines, Singapore\"\n",
    "START_POINT = (1.3533, 103.9452) # Tampines MRT\n",
    "END_POINT = (1.3598, 103.9351)   # Tampines Eco Green\n",
    "\n",
    "# 2. GITHUB DATA LAKE (Edit these to match your repo!)\n",
    "GITHUB_USER = \"swaminaathakrishnan\"\n",
    "REPO_NAME = \"Cool_Route_prototype\"\n",
    "BASE_URL = f\"https://raw.githubusercontent.com/{GITHUB_USER}/{REPO_NAME}/master/data/\"\n",
    "\n",
    "# File Links\n",
    "PCN_URL = BASE_URL + \"ParkConnectorLoop.geojson\"\n",
    "HAWKER_URL = BASE_URL + \"HawkerCentresGEOJSON.geojson\"\n",
    "TREES_URL = BASE_URL + \"trees.csv\" # The new SGTrees Data\n",
    "\n",
    "# 3. SAFETY OVERRIDE (When Govt. advisory is issued)\n",
    "NEA_HEATWAVE_ALERT = False # Set True to force extreme caution\n",
    "\n",
    "print(f\"‚úÖ Configuration Loaded. Connecting to Data Lake at: {GITHUB_USER}/{REPO_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15vvFycdD22Q"
   },
   "source": [
    "### üó∫Ô∏è Module 3: The Spatial Graph Engine (With SGTrees)\n",
    "\n",
    "Objective: Build the road network and overlay specific cooling features. _Upgrade: Now includes Individual Tree Analysis._\n",
    "\n",
    "Layer 1: Road Network (OSM).\n",
    "\n",
    "Layer 2: Park Connectors (PCN).\n",
    "\n",
    "Layer 3: Individual Trees (SGTrees). Logic: Roads with high tree density receive a \"Shade Bonus,\" lowering their travel cost significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kyFS16t9D5dx"
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üó∫Ô∏è MODULE 3: SPATIAL GRAPH ENGINE (V3.2 - CROSS-PLATFORM FIX)\n",
    "# ==========================================\n",
    "import os\n",
    "\n",
    "def generate_cool_routes():\n",
    "    print(f\"‚è≥ Downloading road network for {PLACE_NAME}...\")\n",
    "\n",
    "    # 1. GET GRAPH\n",
    "    G = ox.graph_from_point(START_POINT, dist=2000, network_type='bike')\n",
    "    nodes = ox.graph_to_gdfs(G, edges=False)\n",
    "    miny, maxy = nodes.y.min(), nodes.y.max()\n",
    "    minx, maxx = nodes.x.min(), nodes.x.max()\n",
    "    print(f\"   üìê Zone Limits: Lat[{miny:.4f}, {maxy:.4f}], Lon[{minx:.4f}, {maxx:.4f}]\")\n",
    "\n",
    "    # 2. LOAD PCN DATA\n",
    "    print(\"‚è≥ Overlaying Park Connectors...\")\n",
    "    try:\n",
    "        pcn_data = gpd.read_file(PCN_URL)\n",
    "        if pcn_data.crs != \"EPSG:4326\": pcn_data = pcn_data.to_crs(\"EPSG:4326\")\n",
    "        # FIX: Updated deprecated unary_union to union_all() if available, else unary_union\n",
    "        try: pcn_union = pcn_data.geometry.union_all()\n",
    "        except: pcn_union = pcn_data.geometry.unary_union\n",
    "    except:\n",
    "        print(\"   ‚ö†Ô∏è PCN Data missing. Proceeding without it.\")\n",
    "        pcn_union = None\n",
    "\n",
    "    # 3. LOAD SGTREES DATA (CROSS-PLATFORM FIX)\n",
    "    print(\"‚è≥ Loading Tree Data (Force Download)...\")\n",
    "    trees_buffer = None\n",
    "    local_tree_file = \"trees_downloaded.csv\"\n",
    "\n",
    "    try:\n",
    "        # A. Force Download using requests (Works on all platforms)\n",
    "        lfs_url = f\"https://github.com/{GITHUB_USER}/{REPO_NAME}/raw/master/data/trees.csv\"\n",
    "        print(f\"   üì• Downloading from GitHub LFS...\")\n",
    "        response = requests.get(lfs_url, timeout=30, stream=True)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Write to file\n",
    "        with open(local_tree_file, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"   ‚úÖ Downloaded {os.path.getsize(local_tree_file)/1024/1024:.1f} MB\")\n",
    "\n",
    "        # B. Read the local file\n",
    "        trees_df = pd.read_csv(local_tree_file)\n",
    "\n",
    "        # C. Check if we still got the pointer (The \"4KB trap\")\n",
    "        if len(trees_df) < 50 and \"version https\" in str(trees_df.iloc[0]):\n",
    "            raise ValueError(\"LFS Pointer detected! The 48MB file did not download.\")\n",
    "\n",
    "        # D. Normalize Columns\n",
    "        cols = [c.lower() for c in trees_df.columns]\n",
    "        trees_df.columns = cols\n",
    "        lat_col = 'latitude' if 'latitude' in cols else 'lat'\n",
    "        lng_col = 'longitude' if 'longitude' in cols else 'lng'\n",
    "\n",
    "        # E. Bounding Box Filter\n",
    "        trees_df = trees_df[\n",
    "            (trees_df[lat_col] >= miny) & (trees_df[lat_col] <= maxy) &\n",
    "            (trees_df[lng_col] >= minx) & (trees_df[lng_col] <= maxx)\n",
    "        ]\n",
    "\n",
    "        print(f\"   ‚úÇÔ∏è Filtered Trees: Keeping {len(trees_df)} trees for {PLACE_NAME}.\")\n",
    "\n",
    "        if len(trees_df) > 0:\n",
    "            geometry = [Point(xy) for xy in zip(trees_df[lng_col], trees_df[lat_col])]\n",
    "            trees_gdf = gpd.GeoDataFrame(trees_df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "            \n",
    "            # Project to metric CRS for accurate buffering (Singapore: EPSG:3414)\n",
    "            trees_gdf_proj = trees_gdf.to_crs(\"EPSG:3414\")\n",
    "            trees_buffer_proj = trees_gdf_proj.geometry.buffer(10).unary_union  # 10 meters\n",
    "            # Convert back to WGS84\n",
    "            trees_buffer = gpd.GeoSeries([trees_buffer_proj], crs=\"EPSG:3414\").to_crs(\"EPSG:4326\")[0]\n",
    "            print(f\"   ‚úÖ Shade Layer Generated (10m radius).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Tree Data Error: {e}. Skipping micro-shade analysis.\")\n",
    "\n",
    "    # 4. LOAD SHELTERS\n",
    "    shelters = []\n",
    "    try:\n",
    "        hawker_data = gpd.read_file(HAWKER_URL)\n",
    "        hawker_data = hawker_data.cx[minx:maxx, miny:maxy]\n",
    "        for _, row in hawker_data.iterrows():\n",
    "            name = row.get('Name') or row.get('NAME') or 'Shelter'\n",
    "            shelters.append((name, row.geometry.y, row.geometry.x))\n",
    "    except: pass\n",
    "\n",
    "    # 5. CALCULATE COST\n",
    "    print(\"‚è≥ Calculating 'Micro-Shade' Scores...\")\n",
    "    for u, v, k, data in G.edges(keys=True, data=True):\n",
    "        if 'geometry' in data:\n",
    "            edge_geom = data['geometry']\n",
    "        else:\n",
    "            edge_geom = LineString([(G.nodes[u]['x'], G.nodes[u]['y']), (G.nodes[v]['x'], G.nodes[v]['y'])])\n",
    "\n",
    "        cost = data['length']\n",
    "        is_pcn = False\n",
    "        if pcn_union and edge_geom.intersects(pcn_union):\n",
    "            is_pcn = True\n",
    "            cost *= 0.5\n",
    "\n",
    "        has_shade = False\n",
    "        if trees_buffer and edge_geom.intersects(trees_buffer):\n",
    "            has_shade = True\n",
    "            cost *= 0.6\n",
    "\n",
    "        data['cool_cost'] = cost\n",
    "        data['tag'] = \"üåø PCN + üå≥ Canopy\" if (is_pcn and has_shade) else \"üåø PCN\" if is_pcn else \"üå≥ Shaded Road\" if has_shade else \"‚òÄÔ∏è Exposed\"\n",
    "\n",
    "    # 6. SOLVE\n",
    "    orig = ox.distance.nearest_nodes(G, START_POINT[1], START_POINT[0])\n",
    "    dest = ox.distance.nearest_nodes(G, END_POINT[1], END_POINT[0])\n",
    "\n",
    "    try:\n",
    "        r_fast = nx.shortest_path(G, orig, dest, weight='length')\n",
    "        r_cool = nx.shortest_path(G, orig, dest, weight='cool_cost')\n",
    "        return G, r_fast, r_cool, shelters\n",
    "    except:\n",
    "        return None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lORoECPKEX2a"
   },
   "source": [
    "### üß† Module 4: The Historical AI Engine (Self-Healing)\n",
    "\n",
    "Objective: Predict short-term WBGT trends using historical data. Features:\n",
    "\n",
    "Pagination Logic: Fetches huge datasets by turning API pages.\n",
    "\n",
    "Physics Clamping: Prevents unrealistic temperature predictions (>0.5¬∞C swings).\n",
    "\n",
    "Self-Healing Cache: Automatically deletes bad cache files and re-learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MqM53NLEEZ97"
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üß† MODULE 4: HISTORICAL AI ENGINE (V3)\n",
    "# ==========================================\n",
    "CACHE_FILE = \"coolride_weather_memory.pkl\"\n",
    "\n",
    "def get_cache():\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        try:\n",
    "            with open(CACHE_FILE, \"rb\") as f: return pickle.load(f)\n",
    "        except: return {}\n",
    "    return {}\n",
    "\n",
    "def save_cache(data):\n",
    "    with open(CACHE_FILE, \"wb\") as f: pickle.dump(data, f)\n",
    "\n",
    "def fetch_historical_data(station_name, days_back=3):\n",
    "    cache = get_cache()\n",
    "    today_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    cache_key = f\"{station_name}_{today_str}\"\n",
    "\n",
    "    if cache_key in cache and len(cache[cache_key]['values']) > 20:\n",
    "        print(f\"   ‚ö° Memory Hit! Loaded {len(cache[cache_key]['values'])} points.\")\n",
    "        return cache[cache_key]['timestamps'], cache[cache_key]['values']\n",
    "\n",
    "    print(f\"   üì° Memory Miss. Analyzing last {days_back} days...\")\n",
    "    all_timestamps, all_values = [], []\n",
    "\n",
    "    for i in range(days_back + 1):\n",
    "        target_date = (datetime.now() - timedelta(days=i)).strftime(\"%Y-%m-%d\")\n",
    "        url = \"https://api-open.data.gov.sg/v2/real-time/api/weather\"\n",
    "        params = {\"api\": \"wbgt\", \"date\": target_date}\n",
    "\n",
    "        # PAGINATION LOOP\n",
    "        while True:\n",
    "            try:\n",
    "                resp = requests.get(url, params=params, timeout=5)\n",
    "                if resp.status_code != 200: break\n",
    "                data = resp.json()\n",
    "                if 'data' not in data: break\n",
    "\n",
    "                for rec in data['data'].get('records', []):\n",
    "                    dt = datetime.fromisoformat(rec['datetime'])\n",
    "                    for r in rec['item']['readings']:\n",
    "                        if r.get('station', {}).get('name') == station_name:\n",
    "                            val = float(r.get('wbgt') or r.get('value'))\n",
    "                            mins = dt.hour * 60 + dt.minute\n",
    "                            now_mins = datetime.now().hour * 60 + datetime.now().minute\n",
    "                            if abs(mins - now_mins) < 240: # 4 hour window\n",
    "                                all_timestamps.append(mins)\n",
    "                                all_values.append(val)\n",
    "\n",
    "                token = data['data'].get('paginationToken')\n",
    "                if token: params['paginationToken'] = token\n",
    "                else: break\n",
    "            except: break\n",
    "\n",
    "    if len(all_values) > 20:\n",
    "        cache[cache_key] = {'timestamps': all_timestamps, 'values': all_values}\n",
    "        save_cache(cache)\n",
    "        print(f\"   üíæ Learned & Saved {len(all_values)} thermal patterns.\")\n",
    "\n",
    "    return all_timestamps, all_values\n",
    "\n",
    "def predict_trend(station_name, current_wbgt):\n",
    "    timestamps, values = fetch_historical_data(station_name)\n",
    "    if len(values) < 10: return current_wbgt, \"Stable ‚ûñ\", \"Low Data\"\n",
    "\n",
    "    # Linear Regression\n",
    "    model = LinearRegression()\n",
    "    model.fit(np.array(timestamps).reshape(-1, 1), np.array(values))\n",
    "\n",
    "    # Forecast\n",
    "    now = datetime.now()\n",
    "    fut_min = now.hour * 60 + now.minute + 15\n",
    "    raw_pred = model.predict([[fut_min]])[0]\n",
    "\n",
    "    # Physics Clamp\n",
    "    delta = raw_pred - current_wbgt\n",
    "    if abs(delta) > 0.5:\n",
    "        final_pred = current_wbgt + (0.5 if delta > 0 else -0.5)\n",
    "        note = \"(Physics Clamped)\"\n",
    "    else:\n",
    "        final_pred = raw_pred\n",
    "        note = \"\"\n",
    "\n",
    "    trend = \"Rising üìà\" if final_pred > current_wbgt + 0.1 else \"Falling üìâ\" if final_pred < current_wbgt - 0.1 else \"Stable ‚ûñ\"\n",
    "    return final_pred, trend, f\"High {note}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owRqKP4MEvtZ"
   },
   "source": [
    "### üöÄ Module 5: Execution & Safe-Pace Recommendations\n",
    "\n",
    "Objective: Synthesize map, weather, and AI data into a KML route. Upgrade:\n",
    "\n",
    "* Govt Override: Checks NEA_HEATWAVE_ALERT.\n",
    "* Safe Pacing: Suggests specific ride speeds and hydration intervals based on WBGT (ISO 7243 standards)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jbhsl3iKExZF",
    "outputId": "3dfac1f6-fc53-432d-a82b-586ce40a6660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ STARTING COOLRIDE ENGINE (WITH ACTIVE AI)...\n",
      "‚è≥ Downloading road network for Tampines, Singapore...\n",
      "   üìê Zone Limits: Lat[1.3353, 1.3713], Lon[103.9273, 103.9632]\n",
      "‚è≥ Overlaying Park Connectors...\n",
      "‚è≥ Loading Tree Data (Force Download)...\n",
      "   üì• Downloading from GitHub LFS...\n",
      "   ‚úÖ Downloaded 45.4 MB\n",
      "   ‚úÇÔ∏è Filtered Trees: Keeping 29766 trees for Tampines, Singapore.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b_/23pwz6fn369fzvc0hgdvm4lw0000gn/T/ipykernel_96517/53487934.py:73: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  trees_buffer_proj = trees_gdf_proj.geometry.buffer(10).unary_union  # 10 meters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Shade Layer Generated (10m radius).\n",
      "‚è≥ Calculating 'Micro-Shade' Scores...\n",
      "‚è≥ Connecting to NEA Official WBGT Sensor Network...\n",
      "   üìç Nearest Sensor: Bedok North Street 2 (Dist: 3.12 km)\n",
      "   ‚ö° Memory Hit! Loaded 111 points.\n",
      "\n",
      "üìä REPORT: Bedok North Street 2\n",
      "   Current: 28.9¬∞C | Forecast: 28.4¬∞C\n",
      "   üîç Route Similarity Score: 93.1%\n",
      "   üí° Insight: Routes are effectively identical (Merged).\n",
      "\n",
      "üéâ SUCCESS! Download 'output/latest_route.kml'\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# üöÄ MODULE 5: EXECUTION ENGINE (V3.9 - FINAL LABELS)\n",
    "# ==========================================\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import simplekml\n",
    "\n",
    "print(\"üöÄ STARTING COOLRIDE ENGINE (WITH ACTIVE AI)...\")\n",
    "\n",
    "# --- HELPER: DYNAMIC SENSOR FINDER (FULL V2 LOGIC) ---\n",
    "def get_nearest_wbgt_station(lat, lon):\n",
    "    print(\"‚è≥ Connecting to NEA Official WBGT Sensor Network...\")\n",
    "    url = \"https://api-open.data.gov.sg/v2/real-time/api/weather\"\n",
    "    try:\n",
    "        resp = requests.get(url, params={\"api\": \"wbgt\"}, timeout=10)\n",
    "        data = resp.json()\n",
    "        readings = data['data']['records'][0]['item'].get('readings', [])\n",
    "\n",
    "        closest_station = \"Unknown\"\n",
    "        min_dist = float('inf')\n",
    "        current_val = None\n",
    "\n",
    "        for r in readings:\n",
    "            try:\n",
    "                loc = {}\n",
    "                s_name = \"Unknown\"\n",
    "                if 'location' in r: loc = r['location']\n",
    "                elif 'station' in r and 'location' in r['station']: loc = r['station']['location']\n",
    "                if 'station' in r: s_name = r['station'].get('name', 'Unknown')\n",
    "                s_lat = float(loc.get('latitude', 0))\n",
    "                s_lon = float(loc.get('longitude', loc.get('longtitude', 0)))\n",
    "                if s_lat == 0 or s_lon == 0: continue\n",
    "\n",
    "                val = r.get('wbgt') or r.get('value')\n",
    "                if val is None: continue\n",
    "                val = float(val)\n",
    "                dist = math.sqrt((lat - s_lat)**2 + (lon - s_lon)**2)\n",
    "\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    closest_station = s_name\n",
    "                    current_val = val\n",
    "            except: continue\n",
    "\n",
    "        if current_val is None: return 30.0, \"System Fallback\"\n",
    "        print(f\"   üìç Nearest Sensor: {closest_station} (Dist: {min_dist*111:.2f} km)\")\n",
    "        return current_val, closest_station\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è WBGT Sensor Fail: {e}. Using Default Safety Value.\")\n",
    "        return 30.0, \"System Fallback\"\n",
    "\n",
    "# 1. GENERATE ROUTES\n",
    "graph, r1, r2, shelters = generate_cool_routes()\n",
    "\n",
    "if graph:\n",
    "    # 2. GET WEATHER\n",
    "    current_wbgt, station_name = get_nearest_wbgt_station(START_POINT[0], START_POINT[1])\n",
    "\n",
    "    # 3. RUN AI\n",
    "    pred_wbgt, trend, confidence = predict_trend(station_name, current_wbgt)\n",
    "    effective_wbgt = max(current_wbgt, pred_wbgt)\n",
    "    if NEA_HEATWAVE_ALERT: effective_wbgt = 35.0\n",
    "\n",
    "    # 4. REPORT\n",
    "    if effective_wbgt < 29: rec = \"‚úÖ Safe to Ride.\"\n",
    "    elif effective_wbgt < 31: rec = \"‚ö†Ô∏è CAUTION: Seek shade.\"\n",
    "    else: rec = \"üõë HIGH RISK: Stop.\"\n",
    "\n",
    "    print(f\"\\nüìä REPORT: {station_name}\")\n",
    "    print(f\"   Current: {current_wbgt}¬∞C | Forecast: {pred_wbgt:.1f}¬∞C\")\n",
    "\n",
    "    # 5. EXPORT KML (CLEAN LABELS)\n",
    "    kml = simplekml.Kml()\n",
    "\n",
    "    def add_route(route, color, name, description):\n",
    "        ls = kml.newlinestring(name=name)\n",
    "        coords = []\n",
    "        for u, v in zip(route[:-1], route[1:]):\n",
    "            d = graph.get_edge_data(u, v)[0]\n",
    "            if 'geometry' in d:\n",
    "                xs, ys = d['geometry'].xy\n",
    "                coords.extend(list(zip(xs, ys)))\n",
    "            else:\n",
    "                coords.append((graph.nodes[u]['x'], graph.nodes[u]['y']))\n",
    "                coords.append((graph.nodes[v]['x'], graph.nodes[v]['y']))\n",
    "        ls.coords = coords\n",
    "        ls.style.linestyle.color = color\n",
    "        ls.style.linestyle.width = 5\n",
    "        ls.description = description\n",
    "\n",
    "    # üß† FUZZY LOGIC + CLEAN LABELS\n",
    "    def check_similarity(route_a, route_b):\n",
    "        set_a = set(route_a)\n",
    "        set_b = set(route_b)\n",
    "        intersection = len(set_a.intersection(set_b))\n",
    "        union = len(set_a.union(set_b))\n",
    "        return intersection / union\n",
    "\n",
    "    sim_score = check_similarity(r1, r2)\n",
    "    print(f\"   üîç Route Similarity Score: {sim_score*100:.1f}%\")\n",
    "\n",
    "    if sim_score > 0.90:  # If >90% similar, merge them\n",
    "        print(\"   üí° Insight: Routes are effectively identical (Merged).\")\n",
    "        # MERGED LABEL\n",
    "        add_route(r2, simplekml.Color.green, \"üåü Recommended Route\",\n",
    "                  f\"<b>Smart Choice</b><br>The fastest path is also the coolest.<br>No detour needed.<br><br>Temp: {effective_wbgt:.1f}¬∞C\")\n",
    "    else:\n",
    "        print(\"   üí° Insight: A distinct cooler detour exists.\")\n",
    "        # DIVERGENT LABELS\n",
    "        add_route(r1, simplekml.Color.red, \"‚ö° Fastest Route (Exposed)\",\n",
    "                  f\"<b>Direct Path</b><br>Shortest time, but higher heat exposure.<br><br>Temp: {effective_wbgt:.1f}¬∞C\")\n",
    "        add_route(r2, simplekml.Color.green, \"üåø Cool Route (Shaded)\",\n",
    "                  f\"<b>Shaded Detour</b><br>Maximized tree canopy coverage.<br>Lower heat stress.<br><br>Temp: {effective_wbgt:.1f}¬∞C\")\n",
    "\n",
    "    # Add Shelters\n",
    "    if shelters:\n",
    "        for name, lat, lon in shelters:\n",
    "            p = kml.newpoint(name=f\"üßä {name}\", coords=[(lon, lat)])\n",
    "            p.style.iconstyle.icon.href = 'http://googleusercontent.com/maps.google.com/mapfiles/kml/shapes/snowflake_simple.png'\n",
    "\n",
    "    # 6. SAVE\n",
    "    if not os.path.exists('output'): os.makedirs('output')\n",
    "    constant_filename = \"output/latest_route.kml\"\n",
    "    kml.save(constant_filename)\n",
    "\n",
    "    print(f\"\\nüéâ SUCCESS! Download '{constant_filename}'\")\n",
    "else:\n",
    "    print(\"‚ùå Critical Error: Route Generation Failed.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
