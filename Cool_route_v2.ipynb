{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swaminaathakrishnan/MynewRepo/blob/master/Cool_route_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N598BiGCl2Eg",
        "outputId": "2776f904-c7fe-4495-f225-a6f1fee3cd2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting osmnx\n",
            "  Downloading osmnx-2.0.7-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting simplekml\n",
            "  Downloading simplekml-1.3.6.tar.gz (52 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.12/dist-packages (2.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from osmnx) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.4 in /usr/local/lib/python3.12/dist-packages (from osmnx) (2.2.2)\n",
            "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from geopandas) (0.12.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from geopandas) (25.0)\n",
            "Requirement already satisfied: pyproj>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from geopandas) (3.7.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4->osmnx) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4->osmnx) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4->osmnx) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4->osmnx) (1.17.0)\n",
            "Downloading osmnx-2.0.7-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: simplekml\n",
            "  Building wheel for simplekml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for simplekml: filename=simplekml-1.3.6-py3-none-any.whl size=65860 sha256=064810d1a812a5f7d7b079eaa372d07ca584587c8dccdf3a9ab51c1c314c42e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/ee/f2/65cecfd948f1429ead035fd6d56bc6bd6574a636ddc4d65cbd\n",
            "Successfully built simplekml\n",
            "Installing collected packages: simplekml, osmnx\n",
            "Successfully installed osmnx-2.0.7 simplekml-1.3.6\n",
            "‚úÖ Environment Ready! Proceed to next cell.\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# CELL 1: SETUP & INSTALLATION\n",
        "# ==========================================\n",
        "!pip install osmnx simplekml geopandas shapely networkx requests\n",
        "\n",
        "import osmnx as ox\n",
        "import networkx as nx\n",
        "import simplekml\n",
        "import geopandas as gpd\n",
        "import requests\n",
        "import statistics\n",
        "import math\n",
        "from shapely.geometry import Point, LineString\n",
        "\n",
        "print(\"‚úÖ Environment Ready! Proceed to next cell.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# CELL 2: CONFIGURATION\n",
        "# ==========================================\n",
        "# 1. Pilot Zone\n",
        "PLACE_NAME = \"Tampines, Singapore\"\n",
        "\n",
        "# 2. File Paths (Your Google Drive)\n",
        "PCN_PATH = \"/content/drive/MyDrive/YDCT/ParkConnectorLoop.geojson\"\n",
        "HAWKER_PATH = \"/content/drive/MyDrive/YDCT/HawkerCentresGEOJSON.geojson\"\n",
        "\n",
        "# 3. Define Start & End Points (Lat, Long)\n",
        "# Example: Tampines MRT -> Bedok Reservoir Park\n",
        "START_POINT = (1.3533, 103.9452) # Tampines MRT\n",
        "END_POINT = (1.3405, 103.9312)   # Bedok Reservoir Food Centre\n",
        "\n",
        "print(\"‚úÖ Configuration Set.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U53BNTKZl-gt",
        "outputId": "326054c1-8f39-4826-b539-b7a717bbb661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Configuration Set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# CELL 3: THE \"REAL DATA\" ENGINE (AGGRESSIVE)\n",
        "# ==========================================\n",
        "def generate_cool_routes():\n",
        "    print(f\"‚è≥ Downloading road network for {PLACE_NAME}...\")\n",
        "\n",
        "    # 1. GET THE GRAPH\n",
        "    # Dist increased to 3000m to allow wider detours\n",
        "    G = ox.graph_from_point(START_POINT, dist=3000, network_type='bike')\n",
        "\n",
        "    # 2. INJECT PARK CONNECTOR DATA\n",
        "    print(\"‚è≥ Overlaying Government PCN Data...\")\n",
        "    try:\n",
        "        pcn_data = gpd.read_file(PCN_PATH)\n",
        "        if pcn_data.crs != \"EPSG:4326\": pcn_data = pcn_data.to_crs(\"EPSG:4326\")\n",
        "        pcn_union = pcn_data.geometry.unary_union\n",
        "        print(f\"   ‚úÖ Loaded {len(pcn_data)} PCN segments.\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è PCN Error: {e}\")\n",
        "        pcn_union = None\n",
        "\n",
        "    # 3. INJECT HAWKER DATA\n",
        "    print(\"‚è≥ Loading Hawker Shelters...\")\n",
        "    shelters = []\n",
        "    try:\n",
        "        hawker_data = gpd.read_file(HAWKER_PATH)\n",
        "        if hawker_data.crs != \"EPSG:4326\": hawker_data = hawker_data.to_crs(\"EPSG:4326\")\n",
        "        minx, miny, maxx, maxy = ox.utils_geo.bbox_from_point(START_POINT, dist=3000)\n",
        "        hawker_data = hawker_data.cx[miny:maxy, minx:maxx]\n",
        "        for _, row in hawker_data.iterrows():\n",
        "            name = row.get('Name') or row.get('NAME') or \"Cooling Shelter\"\n",
        "            shelters.append((name, row.geometry.y, row.geometry.x))\n",
        "    except: pass\n",
        "\n",
        "    # 4. MAP \"COOL\" SEGMENTS (The Aggressive Update)\n",
        "    print(\"‚è≥ Analyzing Thermal Safety...\")\n",
        "    for u, v, k, data in G.edges(keys=True, data=True):\n",
        "        # We need the geometry to check intersection accurately\n",
        "        if 'geometry' in data:\n",
        "            edge_geom = data['geometry']\n",
        "        else:\n",
        "            # Create straight line if geometry missing\n",
        "            edge_geom = LineString([(G.nodes[u]['x'], G.nodes[u]['y']), (G.nodes[v]['x'], G.nodes[v]['y'])])\n",
        "\n",
        "        # COST LOGIC V2 (Aggressive)\n",
        "        is_cool = False\n",
        "        if pcn_union and (edge_geom.intersects(pcn_union) or edge_geom.distance(pcn_union) < 0.0001):\n",
        "            is_cool = True\n",
        "\n",
        "        # Force the split:\n",
        "        if is_cool:\n",
        "            # Super cheap cost. The algorithm LOVES this.\n",
        "            data['cool_cost'] = data['length'] * 0.1\n",
        "            data['type'] = \"PCN\"\n",
        "        else:\n",
        "            # Massive penalty. The algorithm HATES this.\n",
        "            # 10x penalty means it will detour up to 10x distance just to stay cool.\n",
        "            data['cool_cost'] = data['length'] * 10.0\n",
        "            data['type'] = \"Road\"\n",
        "\n",
        "    # 5. SOLVE PATHS\n",
        "    print(\"‚è≥ Solving Routes...\")\n",
        "    orig = ox.distance.nearest_nodes(G, START_POINT[1], START_POINT[0])\n",
        "    dest = ox.distance.nearest_nodes(G, END_POINT[1], END_POINT[0])\n",
        "\n",
        "    try:\n",
        "        route_fast = nx.shortest_path(G, orig, dest, weight='length')\n",
        "        route_cool = nx.shortest_path(G, orig, dest, weight='cool_cost')\n",
        "\n",
        "        # Check if they are identical (Debugging)\n",
        "        if route_fast == route_cool:\n",
        "            print(\"‚ö†Ô∏è Routes are identical. The destination might be too close or no PCN exists nearby.\")\n",
        "        else:\n",
        "            print(\"‚úÖ Routes Diverged! (Green path found a detour)\")\n",
        "\n",
        "        return G, route_fast, route_cool, shelters\n",
        "    except nx.NetworkXNoPath:\n",
        "        print(\"‚ùå Error: No path found.\")\n",
        "        return None, None, None, None"
      ],
      "metadata": {
        "id": "H4fa5g1-mGVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# CELL 4: WEATHER INTELLIGENCE (FINAL FIX)\n",
        "# ==========================================\n",
        "import requests\n",
        "import math\n",
        "import statistics\n",
        "import simplekml\n",
        "import json\n",
        "\n",
        "def get_nearest_wbgt_station(target_lat, target_lon):\n",
        "    print(\"‚è≥ Connecting to NEA Official WBGT Sensor Network...\")\n",
        "    url = \"https://api-open.data.gov.sg/v2/real-time/api/weather\"\n",
        "    params = {\"api\": \"wbgt\"}\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, params=params, timeout=10)\n",
        "        data = response.json()\n",
        "        readings = data['data']['records'][0]['item']['readings']\n",
        "\n",
        "        nearest_station = \"Unknown\"\n",
        "        min_dist = float('inf')\n",
        "        wbgt_value = None\n",
        "\n",
        "        # DEBUG: Confirming we see data\n",
        "        # print(f\"   üîç RAW SAMPLE: {json.dumps(readings[0], indent=2)}\")\n",
        "\n",
        "        for r in readings:\n",
        "            try:\n",
        "                # 1. Extract Location (FIXED: Location is at root, not inside station)\n",
        "                # Check directly in 'r' first\n",
        "                if 'location' in r:\n",
        "                    loc = r['location']\n",
        "                elif 'station' in r and 'location' in r['station']:\n",
        "                    # Fallback for old API structure\n",
        "                    loc = r['station']['location']\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                s_lat = float(loc.get('latitude', 0))\n",
        "                s_lon = float(loc.get('longitude', loc.get('longtitude', 0))) # Handle typo\n",
        "                s_name = r.get('station', {}).get('name', 'Unknown')\n",
        "\n",
        "                # 2. Extract Value\n",
        "                raw_val = r.get('wbgt') or r.get('value')\n",
        "                if raw_val is None or str(raw_val).strip() == \"-\": continue\n",
        "                s_val = float(raw_val)\n",
        "\n",
        "                # 3. Find Nearest\n",
        "                dist = math.sqrt((target_lat - s_lat)**2 + (target_lon - s_lon)**2)\n",
        "                if dist < min_dist:\n",
        "                    min_dist = dist\n",
        "                    nearest_station = s_name\n",
        "                    wbgt_value = s_val\n",
        "            except: continue\n",
        "\n",
        "        if wbgt_value is None:\n",
        "            return 32.0, \"Simulation\"\n",
        "\n",
        "        print(f\"   üìç Nearest Sensor: {nearest_station} (Dist: {min_dist*111:.2f} km)\")\n",
        "        print(f\"   üå°Ô∏è Official WBGT: {wbgt_value}¬∞C\")\n",
        "        return wbgt_value, nearest_station\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è API Error: {e}. Defaulting to 32¬∞C.\")\n",
        "        return 32.0, \"Simulation\"\n",
        "\n",
        "def get_air_temp():\n",
        "    try:\n",
        "        url = \"https://api-open.data.gov.sg/v2/real-time/api/air-temperature\"\n",
        "        data = requests.get(url, timeout=5).json()\n",
        "        readings = [r['value'] for r in data['data']['readings'][0]['data']]\n",
        "        return statistics.mean(readings)\n",
        "    except:\n",
        "        return 30.0\n",
        "\n",
        "def export_to_kml(G, r_fast, r_cool, shelters):\n",
        "    # 1. FETCH LIVE DATA\n",
        "    wbgt, station_id = get_nearest_wbgt_station(START_POINT[0], START_POINT[1])\n",
        "\n",
        "    if wbgt is None: wbgt = 32.0\n",
        "    air_temp = get_air_temp()\n",
        "\n",
        "    kml = simplekml.Kml()\n",
        "\n",
        "    # 2. RISK LOGIC (FIXED VARIABLE NAME)\n",
        "    if wbgt < 29: risk=\"LOW\"; color_code=\"üü¢\"\n",
        "    elif wbgt < 31: risk=\"MODERATE\"; color_code=\"üü°\"\n",
        "    elif wbgt < 33: risk=\"HIGH\"; color_code=\"üü†\"\n",
        "    else: risk=\"EXTREME\"; color_code=\"üî¥\"\n",
        "\n",
        "    # 3. GEOMETRY HELPER\n",
        "    def get_smooth_coords(graph, route):\n",
        "        coords = []\n",
        "        for u, v in zip(route[:-1], route[1:]):\n",
        "            edge_data = graph.get_edge_data(u, v)[0]\n",
        "            if 'geometry' in edge_data:\n",
        "                xs, ys = edge_data['geometry'].xy\n",
        "                coords.extend(list(zip(xs, ys)))\n",
        "            else:\n",
        "                coords.append((graph.nodes[u]['x'], graph.nodes[u]['y']))\n",
        "                coords.append((graph.nodes[v]['x'], graph.nodes[v]['y']))\n",
        "        return coords\n",
        "\n",
        "    # 4. EXPORT ROUTES\n",
        "    info_block = (\n",
        "        f\"<b>LIVE SENSOR DATA ({station_id})</b><br/>\"\n",
        "        f\"WBGT: {wbgt}¬∞C {color_code}<br/>\"\n",
        "        f\"Heat Risk: {risk}<br/>\"\n",
        "        f\"Air Temp: {air_temp:.1f}¬∞C<br/>\"\n",
        "    )\n",
        "\n",
        "    # Route A: Fast (Red)\n",
        "    ls_fast = kml.newlinestring(name=f\"Fastest Route (High Exposure)\")\n",
        "    ls_fast.coords = get_smooth_coords(G, r_fast)\n",
        "    ls_fast.style.linestyle.color = simplekml.Color.red\n",
        "    ls_fast.style.linestyle.width = 5\n",
        "    ls_fast.description = info_block + \"Path: Direct Road (High Exposure)<br/>Avg Radiant Load: High\"\n",
        "\n",
        "    # Route B: Cool (Green)\n",
        "    ls_cool = kml.newlinestring(name=\"CoolRoute (Shade Optimized)\")\n",
        "    ls_cool.coords = get_smooth_coords(G, r_cool)\n",
        "    ls_cool.style.linestyle.color = simplekml.Color.green\n",
        "    ls_cool.style.linestyle.width = 5\n",
        "    ls_cool.description = info_block + \"Path: PCN/Park Network (Shaded)<br/>Avg Radiant Load: Low\"\n",
        "\n",
        "    # 5. SHELTERS\n",
        "    for name, lat, lon in shelters:\n",
        "        p = kml.newpoint(name=f\"üßä {name}\", coords=[(lon, lat)])\n",
        "        p.style.iconstyle.icon.href = 'http://googleusercontent.com/maps.google.com/mapfiles/kml/shapes/snowflake_simple.png'\n",
        "\n",
        "    filename = f\"CoolRoute_Official_WBGT_{str(wbgt).replace('.','_')}.kml\"\n",
        "    kml.save(filename)\n",
        "    print(f\"\\nüéâ SUCCESS! Generated official report: {filename}\")\n",
        "\n",
        "print(\"‚úÖ Real-Time WBGT Engine (Final Fix) Ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hvoo3sJfmIzu",
        "outputId": "3b68ce35-a918-4a41-9d4c-72c0e8537f55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Real-Time WBGT Engine (Final Fix) Ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# CELL 4.5 V9: PAGINATION-ENABLED AI ENGINE\n",
        "# ==========================================\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from datetime import datetime, timedelta\n",
        "import numpy as np\n",
        "import requests\n",
        "import pickle\n",
        "import os\n",
        "import time\n",
        "\n",
        "CACHE_FILE = \"coolride_weather_memory.pkl\"\n",
        "\n",
        "def get_cache():\n",
        "    if os.path.exists(CACHE_FILE):\n",
        "        try:\n",
        "            with open(CACHE_FILE, \"rb\") as f:\n",
        "                return pickle.load(f)\n",
        "        except: return {}\n",
        "    return {}\n",
        "\n",
        "def save_cache(data):\n",
        "    with open(CACHE_FILE, \"wb\") as f:\n",
        "        pickle.dump(data, f)\n",
        "\n",
        "def fetch_historical_data(station_name, days_back=3):\n",
        "    # 1. CHECK CACHE\n",
        "    cache = get_cache()\n",
        "    today_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    cache_key = f\"{station_name}_{today_str}\"\n",
        "\n",
        "    if cache_key in cache:\n",
        "        count = len(cache[cache_key]['values'])\n",
        "        print(f\"   ‚ö° Cache Hit! Loaded {count} historical points.\")\n",
        "        return cache[cache_key]['timestamps'], cache[cache_key]['values']\n",
        "\n",
        "    # 2. FETCH FROM API (WITH PAGINATION)\n",
        "    print(f\"   üì° Cache Miss. Starting {days_back}-Day Historical Analysis...\")\n",
        "    all_timestamps = []\n",
        "    all_values = []\n",
        "\n",
        "    for i in range(days_back + 1):\n",
        "        target_date = (datetime.now() - timedelta(days=i)).strftime(\"%Y-%m-%d\")\n",
        "        url = \"https://api-open.data.gov.sg/v2/real-time/api/weather\"\n",
        "\n",
        "        # Start with no token\n",
        "        params = {\"api\": \"wbgt\", \"date\": target_date}\n",
        "\n",
        "        print(f\"      üëâ Querying {target_date}...\", end=\" \")\n",
        "\n",
        "        day_records = []\n",
        "        page_count = 0\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                # Timeout 5s per page\n",
        "                resp = requests.get(url, params=params, timeout=5)\n",
        "\n",
        "                if resp.status_code != 200: break\n",
        "\n",
        "                data = resp.json()\n",
        "                if 'data' not in data: break\n",
        "\n",
        "                # Add records from this page\n",
        "                new_records = data['data'].get('records', [])\n",
        "                day_records.extend(new_records)\n",
        "                page_count += 1\n",
        "\n",
        "                # STOPPING CONDITION: Check if pagination token exists\n",
        "                token = data['data'].get('paginationToken')\n",
        "                if token:\n",
        "                    params['paginationToken'] = token # Set up next page\n",
        "                    # Safety break to prevent infinite loops during demo (max 50 pages)\n",
        "                    if page_count > 50: break\n",
        "                else:\n",
        "                    break # No more pages\n",
        "\n",
        "            except: break\n",
        "\n",
        "        print(f\"‚úÖ Downloaded {len(day_records)} raw logs (in {page_count} pages).\", end=\" \")\n",
        "\n",
        "        # EXTRACT RELEVANT DATA\n",
        "        day_points = 0\n",
        "        for rec in day_records:\n",
        "            try:\n",
        "                dt = datetime.fromisoformat(rec['datetime'])\n",
        "                for r in rec['item']['readings']:\n",
        "                    name = r.get('station', {}).get('name')\n",
        "                    if name == station_name:\n",
        "                        val = float(r.get('wbgt') or r.get('value'))\n",
        "\n",
        "                        # Time Alignment\n",
        "                        minutes = dt.hour * 60 + dt.minute\n",
        "                        now_minutes = datetime.now().hour * 60 + datetime.now().minute\n",
        "\n",
        "                        # +/- 4 Hours Window\n",
        "                        if abs(minutes - now_minutes) < 240:\n",
        "                            all_timestamps.append(minutes)\n",
        "                            all_values.append(val)\n",
        "                            day_points += 1\n",
        "            except: continue\n",
        "\n",
        "        print(f\"-> Extracted {day_points} relevant.\")\n",
        "\n",
        "    print(f\"   üîç Total Training Data: {len(all_values)} points.\")\n",
        "\n",
        "    # 3. SAVE TO CACHE\n",
        "    if len(all_values) > 10:\n",
        "        cache[cache_key] = {'timestamps': all_timestamps, 'values': all_values}\n",
        "        save_cache(cache)\n",
        "        print(\"   üíæ History saved to local memory.\")\n",
        "    else:\n",
        "        print(\"   ‚ö†Ô∏è Data sparse. Not caching.\")\n",
        "\n",
        "    return all_timestamps, all_values\n",
        "\n",
        "def predict_short_term_trend(station_name, current_wbgt):\n",
        "    print(f\"üß† Training AI Model for: {station_name}...\")\n",
        "\n",
        "    timestamps, values = fetch_historical_data(station_name, days_back=3)\n",
        "\n",
        "    if len(values) < 5:\n",
        "        print(f\"   ‚ö†Ô∏è Fallback: Persistence model.\")\n",
        "        return current_wbgt, \"Stable ‚ûñ\", \"Low (Data Scarcity)\"\n",
        "\n",
        "    # TRAIN\n",
        "    X = np.array(timestamps).reshape(-1, 1)\n",
        "    y = np.array(values)\n",
        "    model = LinearRegression()\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # PREDICT\n",
        "    now = datetime.now()\n",
        "    cur_min = now.hour * 60 + now.minute\n",
        "    raw_pred = model.predict([[cur_min + 15]])[0]\n",
        "\n",
        "    # CLAMP\n",
        "    MAX_CHANGE = 0.5\n",
        "    delta = raw_pred - current_wbgt\n",
        "\n",
        "    if abs(delta) > MAX_CHANGE:\n",
        "        final_pred = current_wbgt + (0.5 if delta > 0 else -0.5)\n",
        "        clamp_note = \"(Clamped)\"\n",
        "    else:\n",
        "        final_pred = raw_pred\n",
        "        clamp_note = \"\"\n",
        "\n",
        "    # STATS\n",
        "    y_pred_hist = model.predict(X)\n",
        "    rmse = np.sqrt(mean_squared_error(y, y_pred_hist))\n",
        "    conf_pct = max(0, 100 - (rmse * 20))\n",
        "\n",
        "    if final_pred > current_wbgt + 0.1: trend = \"Rising üìà\"\n",
        "    elif final_pred < current_wbgt - 0.1: trend = \"Falling üìâ\"\n",
        "    else: trend = \"Stable ‚ûñ\"\n",
        "\n",
        "    confidence_str = f\"{int(conf_pct)}% {clamp_note}\"\n",
        "\n",
        "    print(f\"   üìä AI Model: Linear Regression (n={len(values)})\")\n",
        "    print(f\"   üîÆ Forecast (+15m): {final_pred:.2f}¬∞C ({trend})\")\n",
        "    print(f\"   üõ°Ô∏è Confidence: {confidence_str}\")\n",
        "\n",
        "    return final_pred, trend, confidence_str\n",
        "\n",
        "print(\"‚úÖ Pagination-Enabled AI Engine Ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDetK3aj2y26",
        "outputId": "7d00047c-126c-4234-beba-324678a65ac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Pagination-Enabled AI Engine Ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# DEFENSE IN DEPTH: GOVERNMENT OVERRIDE\n",
        "# ==========================================\n",
        "# Toggle this manually based on NEA News/SMS\n",
        "NEA_HEATWAVE_ALERT = False # Set to True during demo to show \"Fail-Safe\"\n",
        "\n",
        "# ==========================================\n",
        "# CELL 5: EXECUTE PROTOTYPE (ACTIVE SAFETY AI)\n",
        "# ==========================================\n",
        "print(\"üöÄ STARTING COOLRIDE ENGINE (WITH ACTIVE AI)...\")\n",
        "\n",
        "# 1. Generate Routes\n",
        "graph, r1, r2, shelters = generate_cool_routes()\n",
        "\n",
        "if graph and r1 and r2:\n",
        "    # 2. Get Real-Time Data\n",
        "    current_wbgt, station_name = get_nearest_wbgt_station(START_POINT[0], START_POINT[1])\n",
        "\n",
        "    # 3. Run AI Prediction\n",
        "    # We add 'confidence' to the unpacking as per previous update\n",
        "    if current_wbgt:\n",
        "        pred_wbgt, trend, confidence = predict_short_term_trend(station_name, current_wbgt)\n",
        "    else:\n",
        "        current_wbgt = 32.0; pred_wbgt = 32.0; trend = \"Simulated\"; confidence = \"N/A\"\n",
        "\n",
        "    # --- CRITICAL UPDATE: THE \"SO WHAT?\" LOGIC ---\n",
        "    # We route based on the WORST case scenario (Current vs Future)\n",
        "    effective_wbgt = max(current_wbgt, pred_wbgt)\n",
        "\n",
        "    # 2. The Override (During Govt declared emergency time)\n",
        "    if NEA_HEATWAVE_ALERT:\n",
        "        print(\"\\nüö® GOVERNMENT ADVISORY ACTIVE: FORCING MAX SAFETY MODE.\")\n",
        "        print(\"   (Ignoring sensor readings due to national alert)\")\n",
        "        effective_wbgt = 35.0  # Force it to EXTREME RISK\n",
        "        confidence = \"MAX (Gov Advisory)\"\n",
        "\n",
        "    print(f\"\\n‚öñÔ∏è SAFETY DECISION ENGINE:\")\n",
        "    print(f\"   Current: {current_wbgt}¬∞C | Forecast: {pred_wbgt:.1f}¬∞C\")\n",
        "    print(f\"   üëâ System Optimization Target: {effective_wbgt:.1f}¬∞C\")\n",
        "\n",
        "    # 4. Export KML\n",
        "    print(\"\\nüíæ Exporting Intelligent KML...\")\n",
        "    kml = simplekml.Kml()\n",
        "\n",
        "   # Risk Logic now uses EFFECTIVE WBGT\n",
        "    if effective_wbgt < 29: risk=\"LOW\"; color=\"üü¢\"\n",
        "    elif effective_wbgt < 31: risk=\"MODERATE\"; color=\"üü°\"\n",
        "    elif effective_wbgt < 33: risk=\"HIGH\"; color=\"üü†\"\n",
        "    else: risk=\"EXTREME\"; color=\"üî¥\"\n",
        "\n",
        "    # Info Block\n",
        "    info_block = (\n",
        "        f\"<b>INTELLIGENT SENSOR DATA ({station_name})</b><br/>\"\n",
        "        f\"Now: {current_wbgt}¬∞C<br/>\"\n",
        "        f\"<b>Forecast (+15m): {pred_wbgt:.1f}¬∞C {trend}</b><br/>\"\n",
        "        f\"AI Confidence: {confidence}<br/>\"\n",
        "        f\"Risk Level: {risk} {color}<br/>\"\n",
        "    )\n",
        "\n",
        "    # Helper: Smooth Coords\n",
        "    def get_smooth_coords(graph, route):\n",
        "        coords = []\n",
        "        for u, v in zip(route[:-1], route[1:]):\n",
        "            edge_data = graph.get_edge_data(u, v)[0]\n",
        "            if 'geometry' in edge_data:\n",
        "                xs, ys = edge_data['geometry'].xy\n",
        "                coords.extend(list(zip(xs, ys)))\n",
        "            else:\n",
        "                coords.append((graph.nodes[u]['x'], graph.nodes[u]['y']))\n",
        "                coords.append((graph.nodes[v]['x'], graph.nodes[v]['y']))\n",
        "        return coords\n",
        "\n",
        "    # Route A (Red)\n",
        "    ls_fast = kml.newlinestring(name=f\"Fastest Route ({risk})\")\n",
        "    ls_fast.coords = get_smooth_coords(graph, r1)\n",
        "    ls_fast.style.linestyle.color = simplekml.Color.red\n",
        "    ls_fast.style.linestyle.width = 5\n",
        "    ls_fast.description = info_block + \"Path: Direct Road (High Exposure)\"\n",
        "\n",
        "    # Route B (Green)\n",
        "    ls_cool = kml.newlinestring(name=f\"CoolRoute (AI Safety Buffer)\")\n",
        "    ls_cool.coords = get_smooth_coords(graph, r2)\n",
        "    ls_cool.style.linestyle.color = simplekml.Color.green\n",
        "    ls_cool.style.linestyle.width = 5\n",
        "    ls_cool.description = info_block + \"Path: PCN/Park Network (Shaded)\"\n",
        "\n",
        "    # Shelters\n",
        "    for name, lat, lon in shelters:\n",
        "        p = kml.newpoint(name=f\"üßä {name}\", coords=[(lon, lat)])\n",
        "        p.style.iconstyle.icon.href = 'http://googleusercontent.com/maps.google.com/mapfiles/kml/shapes/snowflake_simple.png'\n",
        "\n",
        "    filename = f\"CoolRoute_AI_Safety_{str(effective_wbgt).replace('.','_')}.kml\"\n",
        "    kml.save(filename)\n",
        "    print(f\"\\nüéâ SUCCESS! Generated AI-Active report: {filename}\")\n",
        "else:\n",
        "    print(\"‚ùå Route Generation Failed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY8uWdHIoGFs",
        "outputId": "f035bc5d-fd1e-404b-9a18-2579426dd0b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ STARTING COOLRIDE ENGINE (WITH ACTIVE AI)...\n",
            "‚è≥ Downloading road network for Tampines, Singapore...\n",
            "‚è≥ Overlaying Government PCN Data...\n",
            "   ‚úÖ Loaded 883 PCN segments.\n",
            "‚è≥ Loading Hawker Shelters...\n",
            "‚è≥ Analyzing Thermal Safety...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-358972399.py:16: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
            "  pcn_union = pcn_data.geometry.unary_union\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Solving Routes...\n",
            "‚úÖ Routes Diverged! (Green path found a detour)\n",
            "‚è≥ Connecting to NEA Official WBGT Sensor Network...\n",
            "   üìç Nearest Sensor: Bedok North Street 2 (Dist: 3.12 km)\n",
            "   üå°Ô∏è Official WBGT: 25.7¬∞C\n",
            "üß† Training AI Model for: Bedok North Street 2...\n",
            "   ‚ö° Cache Hit! Loaded 128 historical points.\n",
            "   üìä AI Model: Linear Regression (n=128)\n",
            "   üîÆ Forecast (+15m): 25.20¬∞C (Falling üìâ)\n",
            "   üõ°Ô∏è Confidence: 75% (Clamped)\n",
            "\n",
            "‚öñÔ∏è SAFETY DECISION ENGINE:\n",
            "   Current: 25.7¬∞C | Forecast: 25.2¬∞C\n",
            "   üëâ System Optimization Target: 25.7¬∞C\n",
            "\n",
            "üíæ Exporting Intelligent KML...\n",
            "\n",
            "üéâ SUCCESS! Generated AI-Active report: CoolRoute_AI_Safety_25_7.kml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if os.path.exists(\"coolride_weather_memory.pkl\"):\n",
        "    os.remove(\"coolride_weather_memory.pkl\")\n",
        "    print(\"üóëÔ∏è Cache Deleted! The AI is now forced to re-learn.\")\n",
        "else:\n",
        "    print(\"‚úÖ Cache already clear.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2lKalz1108Z",
        "outputId": "24659af2-4710-42c6-845b-7ab2a5ecd86b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üóëÔ∏è Cache Deleted! The AI is now forced to re-learn.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UCPQZ4yIq6l0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}